{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e7023a",
   "metadata": {},
   "source": [
    "# CS 585 - Homework 3\n",
    "\n",
    "Tania Soutonglang\n",
    "A20439949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b10b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from  nltk import bigrams as nltk_bigrams\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9742a8",
   "metadata": {},
   "source": [
    "## Problem 1 - Reading the Data\n",
    "\n",
    "Read in file \"train.tsv\" from the Stanford Sentiment Treebank (SST) as shared in the GLUE task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst = pd.read_csv(\"./SST-2/train.tsv\",delimiter=\"\\t\")\n",
    "df_sst.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b02184",
   "metadata": {},
   "source": [
    "Next, split your dataset into train, test, and validation datasets with these sizes (Note that 100 is a small size for test and validation datasets; it was selected to simplify this homework):\n",
    "- Validation: 100 rows\n",
    "- Test: 100 rows\n",
    "- Training: All remaining rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4194d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_sst, test_size = 100/len(df_sst), random_state = 42)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 100/len(train_df), random_state = 42)\n",
    "\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "test_df = test_df.reset_index(drop = True)\n",
    "val_df = val_df.reset_index(drop = True)\n",
    "\n",
    "print(\"Training size: \", len(train_df))\n",
    "print(\"Testing size: \", len(test_df))\n",
    "print(\"Validation size: \", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1cacd3",
   "metadata": {},
   "source": [
    "Review the column \"label\" which indicates positive=1 or negative=0 sentiment. What is the prior probability of each class on your training set? Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa81965",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pos = sum(train_df.label == 1) / len(train_df)\n",
    "print(\"Positive labels: \", round(prior_pos, 2))\n",
    "\n",
    "prior_neg = sum(train_df.label == 0) / len(train_df)\n",
    "print(\"Negative labels: \", round(prior_neg, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0282b",
   "metadata": {},
   "source": [
    "## Problem 2 - Tokenizing Data\n",
    "\n",
    "Write a function that takes a sentence as input, represented as a string, and converts it to a tokenized sequence padded by start and end symbols. For example, \"hello class\" would be converted to: ['\\<s>', 'hello', 'class', '\\</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)     # tokenize sentence\n",
    "    tokens = ['<s>'] + tokens + ['</s>']      # add padding\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77d8b",
   "metadata": {},
   "source": [
    "Apply your function to all sentences in your training set. Show the tokenization of the first sentence of your training set in your notebook output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tokenized = train_df['sentence'].apply(tokenize)\n",
    "\n",
    "print(train_df_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686970b",
   "metadata": {},
   "source": [
    "What is the vocabulary size of your training set? Include your start and end symbol in your vocabulary. Show your result in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all words in the document\n",
    "temp = []\n",
    "for words in train_df_tokenized:\n",
    "    temp.extend(words)\n",
    "\n",
    "# keep only the unique words\n",
    "vocab = np.unique(temp)\n",
    "\n",
    "print(\"Vocabulary size: \", len(vocab))\n",
    "print(\"Start symbol: \", vocab[0])\n",
    "print(\"End symbol: \", vocab[len(vocab) - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3459e90",
   "metadata": {},
   "source": [
    "## Problem 3 - Bigram Counts\n",
    "\n",
    "Write a function that takes an array of tokenized sequences as input (i.e., a list of lists) and counts bigram frequencies in that dataset. \n",
    "\n",
    "Your function should return a two-level dictionary (dictionary of dictionaries) or similar data structure, where the value at index [wi][wj] gives the frequency count of bigram (wi, wj). \n",
    "\n",
    "For example, this expression would give the counts of the bigram \"academy award\": `bigram_counts[\"academy\"][\"award\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5926117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(tokens):\n",
    "    # create a list of bigrams from the tokens\n",
    "    bigrams = []\n",
    "    for sentence in tokens:\n",
    "        bigrams += list(nltk_bigrams(sentence))\n",
    "        \n",
    "    # find the frequency distribution\n",
    "    bigramsFreq = nltk.FreqDist(bigrams)\n",
    "    \n",
    "    # create the dictionary\n",
    "    freqDict = { }\n",
    "\n",
    "    # iterate through list of bigrams\n",
    "    for w in range(len(bigrams)):\n",
    "        # check if key already exists so it doesn't reinit a dict\n",
    "        if bigrams[w][0] in freqDict.keys():\n",
    "            freqDict[bigrams[w][0]][bigrams[w][1]] = bigramsFreq[(bigrams[w][0], bigrams[w][1])]\n",
    "        else: # make a new dict with a new key\n",
    "            freqDict[bigrams[w][0]] = {}\n",
    "            freqDict[bigrams[w][0]][bigrams[w][1]] = bigramsFreq[(bigrams[w][0], bigrams[w][1])]\n",
    "    \n",
    "    # return the two-level dictionary\n",
    "    return freqDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a33ac",
   "metadata": {},
   "source": [
    "Apply your function to the output of problem 2. You should build one counter that represents all sentences in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc090c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = get_frequency(train_df_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38365d4",
   "metadata": {},
   "source": [
    "Use this result to show how many times a sentence starts with \"the\". That is, how often do you see the bigram (\"\\<s>\",\"the\") in your training set? Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts[\"<s>\"][\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acbb46",
   "metadata": {},
   "source": [
    "## Problem 4 - Smoothing\n",
    "\n",
    "Write a function that implements formula [6.13] in that E-NLP textbook (page 129, 6.2 Smoothing and discounting). That is, write a function that applies smoothing and returns a (negative) log-probability of a word given the previous word in the sequence. It is suggested that you use these parameters:\n",
    "- The current word, wm\n",
    "- The previous word, wm-1\n",
    "- bigram counts (output of Problem 3)\n",
    "- alpha, a smoothing parameter\n",
    "- vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(wm, wm_prev, bigram_counts, alpha, vocab_size):\n",
    "    # get count of wm\n",
    "    # check if it is a key\n",
    "    wm_count = 0\n",
    "    if wm_prev in bigram_counts:\n",
    "        if wm not in bigram_counts[wm_prev]:\n",
    "            wm_count = 0\n",
    "        else:\n",
    "            wm_count = bigram_counts[wm_prev][wm]\n",
    "        \n",
    "    \n",
    "    # get count of wm-1\n",
    "    # check if it is a key\n",
    "    wm_prev_count = 0\n",
    "    if bigram_counts[wm_prev]:\n",
    "        for w in bigram_counts[wm_prev]:\n",
    "            wm_prev_count += bigram_counts[wm_prev][w]\n",
    "    \n",
    "    # count(wm-1, wm) + alpha\n",
    "    num = wm_count + alpha\n",
    "    \n",
    "    # count(wm-1) + alpha * vocabulary_size\n",
    "    denom = wm_prev_count + alpha * vocab_size\n",
    "    \n",
    "    # calc smoothed probability\n",
    "    smoothed = num / denom\n",
    "    \n",
    "    # calc log-prob\n",
    "    log_prob = math.log(smoothed)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"apple\" in bigram_counts:\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e4f66",
   "metadata": {},
   "source": [
    "Using this function to show the log probability that the word \"academy\" will be followed by the word \"award\". Try this with alpha=0.001 and alpha=0.5 (you should see very different results!). Show your results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = smooth(\"award\", \"academy\", bigram_counts, alpha=0.001, vocab_size=len(vocab))\n",
    "print('(academy, award)\\tlog probability: ', a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = smooth(\"award\", \"academy\", bigram_counts, alpha=0.5, vocab_size=len(vocab))\n",
    "print('(academy, award)\\tlog probability: ', a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bc9c4",
   "metadata": {},
   "source": [
    "## Problem 5 - Sentence Log-Probability\n",
    "\n",
    "Write a function that returns the log-probability of a sentence which is expected to be a negative number. To do this, assume that the probability of a word in a sequence only depends on the previous word. It is suggested that you use these parameters:\n",
    "- A sentence represented as a single python string\n",
    "- bigram counts (output of Problem 3)\n",
    "- alpha, a smoothing parameter\n",
    "- vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob(sentence, bigram_counts, alpha, vocab_size):\n",
    "    # normalize sentence\n",
    "    tokens = tokenize(sentence)\n",
    "    \n",
    "    # create a list of bigrams from the tokens\n",
    "    bigrams = []\n",
    "    bigrams += list(nltk_bigrams(tokens))\n",
    "    \n",
    "    log_prob = 0\n",
    "    # find bigram in bigram_counts\n",
    "    for w in range(len(bigrams)):\n",
    "        wm_prev = bigrams[w][0]\n",
    "        wm = bigrams[w][1]\n",
    "        \n",
    "        # check if it is a key\n",
    "        if wm_prev in bigram_counts:\n",
    "            if wm not in bigram_counts[wm_prev]:\n",
    "                log_prob += 0\n",
    "            else:\n",
    "                log_prob += smooth(wm, wm_prev, bigram_counts, alpha, vocab_size)  \n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f87f56",
   "metadata": {},
   "source": [
    "Use your function to compute the log probability of these two sentences (Note that the 2nd is not natural English, so it should have a lower (more negative) result that the first):\n",
    "- \"this was a really great movie but it was a little too long.\"\n",
    "- \"long too little a was it but movie great really a was this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sentence_log_prob(\"this was a really great movie but it was a little too long.\", bigram_counts, alpha=1.0, vocab_size=len(vocab))\n",
    "print(\"\\\"this was a really great movie but it was a little too long.\\\" \\tlog probability: \", s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351de760",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = sentence_log_prob(\"long too little a was it but movie great really a was this.\", bigram_counts, alpha=1.0, vocab_size=len(vocab))\n",
    "print(\"\\\"long too little a was it but movie great really a was this.\\\" \\tlog probability: \", s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83e837",
   "metadata": {},
   "source": [
    "## Problem 6 - Tuning Alpha\n",
    "\n",
    "Next, use your validation set to select a good value for \"alpha\".\n",
    "\n",
    "Apply the function you wrote in Problem 5 to your validation dataset using 3 different values of \"alpha\", such as (0.001, 0.01, 0.1). For each value, show the log-likelihood estimate of the validation set. That is, in your notebook show the sum of the log probabilities of all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccc9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_prob_a1 = 0\n",
    "for sentence in val_df['sentence']:\n",
    "    val_prob_a1 += sentence_log_prob(sentence, bigram_counts, alpha=0.001, vocab_size=len(vocab))\n",
    "\n",
    "print(\"alpha = 0.001\\tlog probability: \", val_prob_a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18aabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prob_a2 = 0\n",
    "for sentence in val_df['sentence']:\n",
    "    val_prob_a2 += sentence_log_prob(sentence, bigram_counts, alpha=0.01, vocab_size=len(vocab))\n",
    "\n",
    "print(\"alpha = 0.01\\tlog probability: \", val_prob_a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe76c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prob_a3 = 0\n",
    "for sentence in val_df['sentence']:\n",
    "    val_prob_a3 += sentence_log_prob(sentence, bigram_counts, alpha=0.1, vocab_size=len(vocab))\n",
    "\n",
    "print(\"alpha = 0.1\\tlog probability: \", val_prob_a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74205604",
   "metadata": {},
   "source": [
    "Which alpha gives you the best result? To indicate your selection to the grader, save your selected value to a variable named \"selected_alpha\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b34bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_alpha = 0.001\n",
    "\n",
    "print(\"Having an alpha of {} gave me the best log-likelihood estimate of the validation set.\".format(selected_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c08c1e",
   "metadata": {},
   "source": [
    "## Problem 7 - Applying Language Models\n",
    "\n",
    "In this problem, you will classify your test set of 100 sentences by sentiment, by applying your work from previous problems and modeling the language of both positive and negative sentiment. To do this, you can follow these steps:\n",
    "- Separate your training dataset into positive and negative sentences, and compute vocabulary size and bigram counts for both datasets.\n",
    "- For each of the 100 sentences in your test set:\n",
    "  - Compute both a \"positive sentiment score\" and a \"negative sentiment score\" using (1) the function you wrote in Problem 5, (2) Bayes rule, and (3) class priors as computed in Problem 1.\n",
    "  - Compare these scores to assign a predicted sentiment label to the sentence.\n",
    "- What is the class distribution of your predicted label? That is, how often did your method predict positive sentiment, correctly or incorrectly? How often did it predict negative sentiment? Show results in your notebook.\n",
    "- Compare your predicted label to the true sentiment label. What is the accuracy of this experiment? That is, how often did the true and predicted label match on the test set? Show results in your notebook.\n",
    "\n",
    "For this problem, you do not need to re-tune alpha for your positive and negative datasets (although it\n",
    "may be a good idea to do so), you can re-use the value selected in Problem 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the training dataset into positive and negative sentences\n",
    "\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if train_df[\"label\"][i] == 1:\n",
    "        train_pos.append(train_df[\"sentence\"][i])\n",
    "    elif train_df[\"label\"][i] == 0:\n",
    "        train_neg.append(train_df[\"sentence\"][i])\n",
    "        \n",
    "train_pos = pd.Series(train_pos)\n",
    "train_neg = pd.Series(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the vocabulary size and bigram counts of positive dataset\n",
    "\n",
    "# tokenize\n",
    "train_pos_tokenized = train_pos.apply(tokenize)\n",
    "\n",
    "# get vocabulary\n",
    "temp = []\n",
    "for words in train_pos_tokenized:\n",
    "    temp.extend(words)\n",
    "vocab_pos = np.unique(temp)\n",
    "\n",
    "# get bigram counts\n",
    "bigram_counts_pos = get_frequency(train_pos_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa3d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the vocabulary size and bigram counts of negative dataset\n",
    "\n",
    "# tokenize\n",
    "train_neg_tokenized = train_neg.apply(tokenize)\n",
    "\n",
    "# get vocabulary\n",
    "temp = []\n",
    "for words in train_neg_tokenized:\n",
    "    temp.extend(words)\n",
    "vocab_neg = np.unique(temp)\n",
    "\n",
    "# get bigram counts\n",
    "bigram_counts_neg = get_frequency(train_neg_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive vocabulary size: \", len(vocab_pos))\n",
    "print(\"Negative vocabulary size: \", len(vocab_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(sentence, vocab_pos, vocab_neg):\n",
    "    # calculate positive log-likelihood\n",
    "    log_prob_pos = 0\n",
    "    log_prob_pos += sentence_log_prob(sentence, bigram_counts_pos, alpha=selected_alpha, vocab_size=len(vocab_pos))\n",
    "\n",
    "    # negative log-likelihood\n",
    "    log_prob_neg = 0\n",
    "    log_prob_neg += sentence_log_prob(sentence, bigram_counts_neg, alpha=selected_alpha, vocab_size=len(vocab_neg))\n",
    "    \n",
    "    # apply Bayes rule\n",
    "    posterior_pos = prior_pos * log_prob_pos\n",
    "    posterior_neg = prior_neg * log_prob_neg\n",
    "    \n",
    "    # normalize\n",
    "    posterior_total = posterior_pos + posterior_neg\n",
    "    if posterior_total > 0:\n",
    "        posterior_pos /= posterior_total\n",
    "        posterior_neg /= posterior_total\n",
    "    \n",
    "    if posterior_pos < posterior_neg:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15125e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for sentence in test_df[\"sentence\"]:\n",
    "    test_pred.append(sentiment_analysis(sentence, vocab_pos, vocab_neg))\n",
    "test_pred = pd.Series(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d47b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pos = 0\n",
    "correct_neg = 0\n",
    "incorrect_pos = 0\n",
    "incorrect_neg = 0\n",
    "\n",
    "for i in range(100):\n",
    "    if test_df[\"label\"][i] == 1 and test_pred[i] == 1:\n",
    "        correct_pos += 1\n",
    "    elif test_df[\"label\"][i] == 0 and test_pred[i] == 1:\n",
    "        incorrect_pos += 1\n",
    "    elif test_df[\"label\"][i] == 0 and test_pred[i] == 0:\n",
    "        correct_neg += 1\n",
    "    elif test_df[\"label\"][i] == 1 and test_pred[i] == 0:\n",
    "        incorrect_neg += 1\n",
    "\n",
    "print(\"My method counted {} positive sentiments correctly and {} incorrectly.\".format(correct_pos, incorrect_pos))\n",
    "print(\"My method counted {} negative sentiments correctly and {} incorrectly.\".format(correct_neg, incorrect_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_df[\"label\"], test_pred)\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350afa6f",
   "metadata": {},
   "source": [
    "## Problem 8 - Markov Assumption\n",
    "\n",
    "- Where in this homework did you apply the Markov assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b395f5",
   "metadata": {},
   "source": [
    "The Markov assumption was applied in problem 4 where we predicted the likelihood of the word coming after a certain word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ae02d",
   "metadata": {},
   "source": [
    "- Imagine you applied the 2nd-order Markov assumption, using trigrams. Do you think your accuracy results would increase or decrease? Why? Or, if you are not sure, give a benefit or drawback of using trigrams for this experiment. (Note: You do not need to rerun this experiment with trigrams to answer this question.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0cf35",
   "metadata": {},
   "source": [
    "I'm not sure how trigrams would affect the accuracy because while trigrams would provide more context and improve accuracy for some cases, it would cause a larger dimensionality and won't be able to generalize the data because it would be more specific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b103c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119aeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
