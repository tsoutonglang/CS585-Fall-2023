{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6aafbfa-2d87-4f34-91fc-6228d87954b9",
   "metadata": {},
   "source": [
    "# CS 585 - HW 1 - Getting Started\n",
    "\n",
    "Tania Soutonglang<br>\n",
    "A20439949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfbd0c8-48c1-4773-ad47-f0bf80f9b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tsout\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#NLTK setup - uncomment and run first time you import NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from csv import QUOTE_NONE\n",
    "\n",
    "import numpy as np\n",
    "from nltk import probability\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada6ae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sst dataset\n",
    "df_sst = pd.read_csv(\"SST-2/train.tsv\",delimiter=\"\\t\")\n",
    "df_sst.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894ae4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What came into force after the new constitutio...</td>\n",
       "      <td>As of that day, the new constitution heralding...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the first major city in the stream of ...</td>\n",
       "      <td>The most important tributaries in this area ar...</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the minimum required if you want to te...</td>\n",
       "      <td>In most provinces a second Bachelor's Degree s...</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           question  \\\n",
       "0      0  What came into force after the new constitutio...   \n",
       "1      1  What is the first major city in the stream of ...   \n",
       "2      2  What is the minimum required if you want to te...   \n",
       "\n",
       "                                            sentence           label  \n",
       "0  As of that day, the new constitution heralding...      entailment  \n",
       "1  The most important tributaries in this area ar...  not_entailment  \n",
       "2  In most provinces a second Bachelor's Degree s...  not_entailment  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import qnli dataset\n",
    "df_qnli = pd.read_csv(\"QNLI/dev.tsv\",delimiter=\"\\t\",quoting=QUOTE_NONE)\n",
    "df_qnli.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676afd0",
   "metadata": {},
   "source": [
    "## Problem 1 - Representing English Text\n",
    "\n",
    "- Read in these two GLUE datasets (see section “DATA” above). Also convert alphabetical characters to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cea4e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         hide new secretions from the parental units \n",
       "1                 contains no wit , only labored gags \n",
       "2    that loves its characters and communicates som...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sst series with only sentence column\n",
    "df_sstData = df_sst[['sentence']].dropna()\n",
    "df_sstData = df_sstData['sentence'].str.lower()\n",
    "df_sstData.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b8ce29-a936-4146-9d83-661495910398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    as of that day, the new constitution heralding...\n",
       "1    the most important tributaries in this area ar...\n",
       "2    in most provinces a second bachelor's degree s...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create qnli series with only sentence column\n",
    "df_qnliData = df_qnli[['sentence']].dropna()\n",
    "df_qnliData = df_qnliData['sentence'].str.lower()\n",
    "df_qnliData.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acc5fb",
   "metadata": {},
   "source": [
    "- Convert each dataset into a single list of tokens by applying the function “word_tokenize()” in the NLTK :: nltk.tokenize package. We will use these lists represent two distributions of English text.\n",
    "- To show you have finished this step, print the first 10 tokens from each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f56ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "'30s\n",
      "'40s\n",
      "'50s\n",
      "'53\n",
      "'60s\n",
      "'70s\n",
      "'80s\n",
      "'90s\n",
      "'d\n"
     ]
    }
   ],
   "source": [
    "sst_tokens = []\n",
    "\n",
    "# create token list for sst data\n",
    "for sentence in df_sstData:\n",
    "    # tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # filter out the punctuation\n",
    "    filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # append tokens to list\n",
    "    sst_tokens.append(filtered_tokens)\n",
    "\n",
    "temp = []\n",
    "for words in sst_tokens:\n",
    "    temp.extend(words)\n",
    "sst_vocab = np.unique(temp)\n",
    "\n",
    "for token in range(10):\n",
    "    print(sst_vocab[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de60e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "'aided\n",
      "'apothecary\n",
      "'bath\n",
      "'bends\n",
      "'bucks\n",
      "'carry\n",
      "'chares\n",
      "'church\n",
      "'comb\n"
     ]
    }
   ],
   "source": [
    "qnli_tokens = []\n",
    "\n",
    "# create token list for qnli data\n",
    "for sentence in df_qnliData:\n",
    "    # tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # filter out the punctuation\n",
    "    filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # append tokens to list\n",
    "    qnli_tokens.append(filtered_tokens)\n",
    "\n",
    "temp = []\n",
    "for words in qnli_tokens:\n",
    "    temp.extend(words)\n",
    "qnli_vocab = np.unique(temp)\n",
    "\n",
    "for token in range(10):\n",
    "    print(qnli_vocab[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483091c2",
   "metadata": {},
   "source": [
    "## Problem 2 - Word Probability\n",
    "\n",
    "- Write a python function that creates a probability distribution from a list of tokens. This function should return a dictionary that maps a token to a probability (I.e., maps a string to a floating-point value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7f58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probDist(tokenList):\n",
    "    # get the total number of tokens\n",
    "    count = len(tokenList)\n",
    "\n",
    "    # create a frequency distribution of the token list\n",
    "    freqDist = probability.FreqDist(tokenList)\n",
    "\n",
    "    # calculate the probability of each token\n",
    "    probDist = {token: freq / count for token, freq in freqDist.items()}\n",
    "\n",
    "    return probDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530c326",
   "metadata": {},
   "source": [
    "- Apply your function to the list created in Problem 1 to create SST and QNLI distributions.\n",
    "- Show that both probability distributions sum to 1, allowing for some small numerical rounding error. Or, if they do not, add a comment in your notebook to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1aa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999999999999606\n"
     ]
    }
   ],
   "source": [
    "sst_probDist = create_probDist(sst_vocab)\n",
    "print(sum(sst_probDist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445e6de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000001854\n"
     ]
    }
   ],
   "source": [
    "qnli_probDist = create_probDist(qnli_vocab)\n",
    "print(sum(qnli_probDist.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb08223",
   "metadata": {},
   "source": [
    "## Problem 3 - Entropy\n",
    "\n",
    "- Write a python function that computes the entropy of a random variable, input as a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904c843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(probDist):\n",
    "    entropy = 0.0\n",
    "\n",
    "    # calculate the entropy of each word\n",
    "    for prob in probDist.values():\n",
    "        if prob > 0:\n",
    "            entropy += -prob * math.log2(prob)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91464d",
   "metadata": {},
   "source": [
    "- Use this function to compute the word-level entropy of SST and QNLI, using the distributions you created in Problem 2. Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147bc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.851944197996536\n"
     ]
    }
   ],
   "source": [
    "sst_entropy = calc_entropy(sst_probDist)\n",
    "print(sst_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c5e93a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.944071453008073\n"
     ]
    }
   ],
   "source": [
    "qnli_entropy = calc_entropy(qnli_probDist)\n",
    "print(qnli_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b0ec5",
   "metadata": {},
   "source": [
    "## Problem 4 - KL Divergence\n",
    "\n",
    "- Write a python function to compute the KL divergence between two probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0812bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_KLDivergence(probDist_a, probDist_b):\n",
    "    kl_divergence = 0.0\n",
    "\n",
    "    for key, prob_a in probDist_a.items():\n",
    "        # Get the corresponding probability from the 2nd list, otherwise default to 0 if it doesn't exist\n",
    "        prob_b = probDist_b.get(key, 0)\n",
    "\n",
    "        # calculate the KL divergence\n",
    "        if prob_b > 0:\n",
    "            kl_divergence += prob_a * math.log2(prob_a / prob_b)\n",
    "\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d88a3",
   "metadata": {},
   "source": [
    "- Apply this function to the distributions you created in Problem 2 to show that KL divergence is not symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1f4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032000918551245115\n"
     ]
    }
   ],
   "source": [
    "sst_KLqnli = calc_KLDivergence(sst_probDist, qnli_probDist)\n",
    "print(sst_KLqnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa0f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.030021291903982113\n"
     ]
    }
   ],
   "source": [
    "qnli_KLsst = calc_KLDivergence(qnli_probDist, sst_probDist)\n",
    "print(qnli_KLsst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7c5b6",
   "metadata": {},
   "source": [
    "## Problem 5 - Entropy Rate\n",
    "\n",
    "- Write a python function that computes the per-word entropy rate of a message relative to a specific probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a12aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perWordEntropyRate(doc, probDist):\n",
    "    # split the inputted document into tokens \n",
    "    tokens = doc.split()\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    token_count = len(words)\n",
    "    \n",
    "    entropy_rate = 0.0\n",
    "\n",
    "    for word in tokens:\n",
    "        # get the probability of the word, otherwise it will be 0\n",
    "        probability = probDist.get(word, 0)\n",
    "        if probability > 0:\n",
    "            entropy_rate += -probability * math.log2(probability)\n",
    "\n",
    "        # find the per-word rate\n",
    "        if token_count > 0:\n",
    "            entropy_rate /= token_count\n",
    "\n",
    "    return entropy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62259d10",
   "metadata": {},
   "source": [
    "- Find a recent movie review online (any website) and compute the entropy rates of this movie review using the distributions you created for both SST and QNLI datasets. Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a36a68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie review of The Nun II from Rotten Tomatoes (https://www.rottentomatoes.com/m/the_nun_ii/reviews)\n",
    "review = \"A narratively bland sequel that grants star Taissa Farmiga a bit more agency (while wasting Storm Reid), The Nun II proves that while there's plenty of box office in The Conjuring Universe, the storytelling techniques are phoning it in.\"\n",
    "review = review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "166d5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.576440220966119e-07\n"
     ]
    }
   ],
   "source": [
    "# per-word entropy using sst probability distribution\n",
    "sst_perWord = calc_perWordEntropyRate(review, sst_probDist)\n",
    "print(sst_perWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26c7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.266264845240931e-07\n"
     ]
    }
   ],
   "source": [
    "# per-word entropy using qnli probability distribution\n",
    "qnli_perWord = calc_perWordEntropyRate(review, qnli_probDist)\n",
    "print(qnli_perWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89980f0",
   "metadata": {},
   "source": [
    "## Problem 6 - Observed Entropy Rate\n",
    "- Refer to your results from Problem 5. Which distribution gives you the lowest entropy rate for your movie review? Does this match what you expected? Why or why not?\n",
    "\n",
    "Entropy rate with SST: 4.836273055258784e-07\n",
    "<br>Entropy rate with QNLI: 4.5657562774588297e-07\n",
    "<br>The QNLI distribution gave the lowest entropy rate for my movie review. I expected this only because the probability distribution of this dataset had lower numbers compared to the SST dataset, therefore anything calculated with that probability distribution would be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942609bc",
   "metadata": {},
   "source": [
    "## Problem 7 – Zero probabilities\n",
    "- Problem 5 required that you handle “zero probabilities” cases, where a token occurred in one dataset but not the other. How did you handle these tokens? (Hint: Dropping the word from both probability distributions is not an ideal solution). \n",
    "\n",
    "I handled these cases by making the probability of the missing token 0.0. This allowed me to still calculate the word if it appeared in one dataset, but also factored in the fact that it does not exist in the other (therefore the 0% possibility in the probability distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0e62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
