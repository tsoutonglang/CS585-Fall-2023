{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 585 - Homework 2\n",
    "Tania Soutonglang<br>\n",
    "A20439949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Reading the Data\n",
    "\n",
    "read in clickbait.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin(file, label):\n",
    "    # read in file\n",
    "    file_text = open(file, 'r', encoding='utf-8')\n",
    "    file_list = file_text.readlines()\n",
    "    file_list = [(x.strip('\\n')) for x in file_list]\n",
    "    # cleaned_text = text.rstrip('\\n')\n",
    "\n",
    "    # add labels\n",
    "    file_list = [(x.lower(), label) for x in file_list]\n",
    "\n",
    "    # turn into dataframe\n",
    "    file_df = pd.DataFrame(file_list, columns = [\"sentence\", \"label\"])\n",
    "\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait\n",
      "                                            sentence  label\n",
      "0  man repairs fence to contain dog, hilarity ens...      1\n",
      "1  long-term marijuana use has one crazy side eff...      1\n",
      "2  the water from his ear trickles into the bucke...      1\n",
      "3  you'll never guess what nick jonas does in the...      1\n",
      "4  how cruise liners fill all their unsold cruise...      1\n",
      "\n",
      "not clickbait\n",
      "                                            sentence  label\n",
      "0  congress slips cisa into a budget bill that's ...      0\n",
      "1                      dui arrest sparks controversy      0\n",
      "2  it’s unconstitutional to ban the homeless from...      0\n",
      "3  a government error just revealed snowden was t...      0\n",
      "4  a toddler got meningitis. his anti-vac parents...      0\n"
     ]
    }
   ],
   "source": [
    "clickbait_df = readin('clickbait.txt', 1)\n",
    "print(\"clickbait\")\n",
    "print(clickbait_df.head())\n",
    "\n",
    "print()\n",
    "\n",
    "notclickbait_df = readin('not-clickbait.txt', 0)\n",
    "print(\"not clickbait\")\n",
    "print(notclickbait_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenating and shuffling the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18 celebrities who might be time travelers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in chhattisgarh, pm modi touches the feet of 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n.j. woman jailed for tossing neighbor's dog i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us releases guantánamo prisoner after 14 years...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best buzzer-beater of the weekend miiiight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0         18 celebrities who might be time travelers      1\n",
       "1  in chhattisgarh, pm modi touches the feet of 1...      0\n",
       "2  n.j. woman jailed for tossing neighbor's dog i...      0\n",
       "3  us releases guantánamo prisoner after 14 years...      0\n",
       "4  the best buzzer-beater of the weekend miiiight...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([clickbait_df, notclickbait_df])\n",
    "all_df = all_df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train, test, and validation datasets<br>\n",
    "72% train, 8% validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  1719 -> 72.0%\n",
      "Test size:   478 -> 20.0%\n",
      "Validation size: 191 -> 8.0%\n"
     ]
    }
   ],
   "source": [
    "n_texts = len(all_df)\n",
    "\n",
    "train_df, test_df = train_test_split(all_df, test_size = 0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 0.1)\n",
    "\n",
    "print(f\"Train size:  {len(train_df)} -> {len(train_df)/n_texts:0.1%}\")\n",
    "print(f\"Test size:   {len(test_df)} -> {len(test_df)/n_texts:0.1%}\")\n",
    "print(f\"Validation size: {len(val_df)} -> {len(val_df)/n_texts:0.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the \"target rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1719, clickbait: 597 -> 34.7% target rate\n",
      "Test size: 478, clickbait: 159 -> 33.3% target rate\n",
      "Validation size: 191, clickbait: 58 -> 30.4% target rate\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_df)}, clickbait: {sum(train_df['label'] == 1)} -> {sum(train_df['label'] == 1)/len(train_df):0.1%} target rate\")\n",
    "print(f\"Test size: {len(test_df)}, clickbait: {sum(test_df['label'] == 1)} -> {sum(test_df['label'] == 1)/len(test_df):0.1%} target rate\")\n",
    "print(f\"Validation size: {len(val_df)}, clickbait: {sum(val_df['label'] == 1)} -> {sum(val_df['label'] == 1)/len(val_df):0.1%} target rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Training a Single Bag-of-Words (BOW) Text Classifier\n",
    "\n",
    "(taken from Scikit-learn Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline(*, texts, labels, min_df = 1, max_df = 0.1, ngram_range = (1,1), alpha = 1.0):\n",
    "    \"\"\" Train a text classifier model given input hyperparameters:\n",
    "      - CountVectorizer: min_df, max_df, ngram_range\n",
    "      - NaiveBayes:      alpha\n",
    "    \"\"\"\n",
    "\n",
    "    # Pipeline Step 1: texts -> BOW vectors\n",
    "    vectorizer = CountVectorizer(min_df = min_df, \n",
    "                                 max_df = max_df,\n",
    "                                 stop_words = \"english\",\n",
    "                                 ngram_range = ngram_range)\n",
    "\n",
    "    # Pipeline Step 2: document vectors -> model score\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "\n",
    "    pipeline = Pipeline(steps = [\n",
    "        (\"vectorizer\",vectorizer),\n",
    "        (\"classifier\",model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(texts,labels)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(max_df=0.1, stop_words='english')),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df.sentence.values\n",
    "y_train = train_df.label.values\n",
    "\n",
    "pipeline = fit_pipeline(texts = X_train, labels = y_train)\n",
    "\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_df.sentence.values\n",
    "y_val = val_df.label.values\n",
    "\n",
    "y_pred_val = pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the precision, recall, and f1-score on the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "Precision: 0.98\n",
      "Recall:    0.96\n",
      "F1:        0.97\n",
      "\n",
      "validation set\n",
      "Precision: 0.91\n",
      "Recall:    0.71\n",
      "F1:        0.80\n"
     ]
    }
   ],
   "source": [
    "train_precision = precision_score(y_train, y_pred_train)\n",
    "train_recall = recall_score(y_train, y_pred_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"training set\")\n",
    "print(f\"Precision: {train_precision:.2f}\")\n",
    "print(f\"Recall:    {train_recall:.2f}\")\n",
    "print(f\"F1:        {train_f1:.2f}\")\n",
    "\n",
    "val_precision = precision_score(y_val, y_pred_val)\n",
    "val_recall = recall_score(y_val, y_pred_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "print(\"\\nvalidation set\")\n",
    "print(f\"Precision: {val_precision:.2f}\")\n",
    "print(f\"Recall:    {val_recall:.2f}\")\n",
    "print(f\"F1:        {val_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Hyperparameter Tuning\n",
    "\n",
    "(taken from Scikit-learn Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParameterGrid({\n",
    "    \"max_df\":       [0.01, 0.1],\n",
    "    'alpha':        [0.1, 0.5, 1.0, 2.0],\n",
    "    \"ngram_range\":  [(1,1),(1,2),(1,3)], \n",
    "})\n",
    "\n",
    "def get_metrics(pipeline, texts, labels):\n",
    "    preds = pipeline.predict(texts)\n",
    "    \n",
    "    pr = precision_score(labels, preds)\n",
    "    re = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"precision\": pr,\n",
    "        \"recall\": re,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of grid points: 24\n",
      "{'alpha': 0.1, 'max_df': 0.01, 'ngram_range': (1, 1)}\n",
      "{'alpha': 0.1, 'max_df': 0.01, 'ngram_range': (1, 2)}\n",
      "{'alpha': 0.1, 'max_df': 0.01, 'ngram_range': (1, 3)}\n",
      "{'alpha': 0.1, 'max_df': 0.1, 'ngram_range': (1, 1)}\n",
      "{'alpha': 0.1, 'max_df': 0.1, 'ngram_range': (1, 2)}\n",
      "{'alpha': 0.1, 'max_df': 0.1, 'ngram_range': (1, 3)}\n",
      "{'alpha': 0.5, 'max_df': 0.01, 'ngram_range': (1, 1)}\n",
      "{'alpha': 0.5, 'max_df': 0.01, 'ngram_range': (1, 2)}\n",
      "{'alpha': 0.5, 'max_df': 0.01, 'ngram_range': (1, 3)}\n",
      "{'alpha': 0.5, 'max_df': 0.1, 'ngram_range': (1, 1)}\n",
      "{'alpha': 0.5, 'max_df': 0.1, 'ngram_range': (1, 2)}\n",
      "{'alpha': 0.5, 'max_df': 0.1, 'ngram_range': (1, 3)}\n",
      "{'alpha': 1.0, 'max_df': 0.01, 'ngram_range': (1, 1)}\n",
      "{'alpha': 1.0, 'max_df': 0.01, 'ngram_range': (1, 2)}\n",
      "{'alpha': 1.0, 'max_df': 0.01, 'ngram_range': (1, 3)}\n",
      "{'alpha': 1.0, 'max_df': 0.1, 'ngram_range': (1, 1)}\n",
      "{'alpha': 1.0, 'max_df': 0.1, 'ngram_range': (1, 2)}\n",
      "{'alpha': 1.0, 'max_df': 0.1, 'ngram_range': (1, 3)}\n",
      "{'alpha': 2.0, 'max_df': 0.01, 'ngram_range': (1, 1)}\n",
      "{'alpha': 2.0, 'max_df': 0.01, 'ngram_range': (1, 2)}\n",
      "{'alpha': 2.0, 'max_df': 0.01, 'ngram_range': (1, 3)}\n",
      "{'alpha': 2.0, 'max_df': 0.1, 'ngram_range': (1, 1)}\n",
      "{'alpha': 2.0, 'max_df': 0.1, 'ngram_range': (1, 2)}\n",
      "{'alpha': 2.0, 'max_df': 0.1, 'ngram_range': (1, 3)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>max_df</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.967797</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>15527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>24690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>15527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>24690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.984020</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.995809</td>\n",
       "      <td>15527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>24690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.776699</td>\n",
       "      <td>0.990764</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.956971</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>15470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>24633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>24633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.997485</td>\n",
       "      <td>24633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.997485</td>\n",
       "      <td>15470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.995809</td>\n",
       "      <td>15470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.964407</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.993277</td>\n",
       "      <td>15527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>24690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.988215</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.980688</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.995809</td>\n",
       "      <td>15470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>24633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  max_df ngram_range  precision    recall        f1  f1_train      K\n",
       "15    1.0    0.10      (1, 1)   0.911111  0.706897  0.796117  0.967797   5213\n",
       "10    0.5    0.10      (1, 2)   0.930233  0.689655  0.792079  0.996644  15527\n",
       "11    0.5    0.10      (1, 3)   0.930233  0.689655  0.792079  0.996644  24690\n",
       "4     0.1    0.10      (1, 2)   0.891304  0.706897  0.788462  0.998325  15527\n",
       "5     0.1    0.10      (1, 3)   0.891304  0.706897  0.788462  0.998325  24690\n",
       "9     0.5    0.10      (1, 1)   0.909091  0.689655  0.784314  0.984020   5213\n",
       "16    1.0    0.10      (1, 2)   0.928571  0.672414  0.780000  0.995809  15527\n",
       "17    1.0    0.10      (1, 3)   0.928571  0.672414  0.780000  0.996644  24690\n",
       "3     0.1    0.10      (1, 1)   0.888889  0.689655  0.776699  0.990764   5213\n",
       "21    2.0    0.10      (1, 1)   0.950000  0.655172  0.775510  0.956971   5213\n",
       "1     0.1    0.01      (1, 2)   0.906977  0.672414  0.772277  0.998325  15470\n",
       "2     0.1    0.01      (1, 3)   0.906977  0.672414  0.772277  0.998325  24633\n",
       "14    1.0    0.01      (1, 3)   0.973684  0.637931  0.770833  0.996644  24633\n",
       "8     0.5    0.01      (1, 3)   0.948718  0.637931  0.762887  0.997485  24633\n",
       "7     0.5    0.01      (1, 2)   0.948718  0.637931  0.762887  0.997485  15470\n",
       "13    1.0    0.01      (1, 2)   0.948718  0.637931  0.762887  0.995809  15470\n",
       "12    1.0    0.01      (1, 1)   0.904762  0.655172  0.760000  0.964407   5159\n",
       "22    2.0    0.10      (1, 2)   0.972973  0.620690  0.757895  0.993277  15527\n",
       "23    2.0    0.10      (1, 3)   0.972973  0.620690  0.757895  0.996644  24690\n",
       "0     0.1    0.01      (1, 1)   0.883721  0.655172  0.752475  0.988215   5159\n",
       "6     0.5    0.01      (1, 1)   0.902439  0.637931  0.747475  0.980688   5159\n",
       "18    2.0    0.01      (1, 1)   0.972222  0.603448  0.744681  0.953608   5159\n",
       "19    2.0    0.01      (1, 2)   1.000000  0.551724  0.711111  0.995809  15470\n",
       "20    2.0    0.01      (1, 3)   1.000000  0.551724  0.711111  0.996644  24633"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"# of grid points: {len(param_grid)}\")\n",
    "\n",
    "results_arr = []\n",
    "\n",
    "for gridpt in param_grid:\n",
    "    print(gridpt)\n",
    "    trained = fit_pipeline(texts=X_train, labels=y_train, **gridpt)\n",
    "    metrics = get_metrics(trained, X_val, y_val)\n",
    "    \n",
    "    # save hyperparams and results\n",
    "    combined_data = {**gridpt, **metrics, \"trained\":trained}\n",
    "    # check for overfitting\n",
    "    combined_data['f1_train'] = f1_score(y_train, trained.predict(X_train))\n",
    "\n",
    "    # vocab size\n",
    "    combined_data['K'] = len(trained[0].vocabulary_)\n",
    "    \n",
    "    results_arr.append(combined_data)\n",
    " \n",
    "results_df = pd.DataFrame(results_arr).sort_values(\"f1\", ascending=False)\n",
    "results_df.drop(\"trained\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 - Model Selection\n",
    "(taken from Scikit-learn Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.1, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(max_df=0.1, stop_words='english')),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_pipeline = results_df.loc[15,\"trained\"]\n",
    "selected_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics {'precision': 0.9794168096054888, 'recall': 0.9564489112227805, 'f1': 0.9677966101694915}\n",
      "Valid metrics {'precision': 0.9111111111111111, 'recall': 0.7068965517241379, 'f1': 0.7961165048543689}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train metrics\", get_metrics(\n",
    "    selected_pipeline,\n",
    "    texts=X_train, \n",
    "    labels=y_train)\n",
    ")\n",
    "\n",
    "print(\"Valid metrics\", get_metrics(\n",
    "    selected_pipeline,\n",
    "    texts=X_val, \n",
    "    labels=y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the selected model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.83\n",
      "Recall:    0.75\n",
      "F1:        0.79\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df.sentence.values\n",
    "y_test = test_df.label.values\n",
    "\n",
    "y_pred_test = selected_pipeline.predict(X_test)\n",
    "\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Precision: {test_precision:.2f}\")\n",
    "print(f\"Recall:    {test_recall:.2f}\")\n",
    "print(f\"F1:        {test_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 - Key Indicators\n",
    "(taken from Scikit-learn Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5213\n",
      "['believe', 'won', 'll', 'new', 'guess']\n"
     ]
    }
   ],
   "source": [
    "selected_vect = selected_pipeline[0]\n",
    "vocab = sorted([v for v in selected_vect.vocabulary_.keys()], key=selected_vect.vocabulary_.get)\n",
    "print(len(vocab))\n",
    "\n",
    "selected_model = selected_pipeline[1]\n",
    "log_probs = selected_model.feature_log_prob_[1:].reshape(-1)\n",
    "v_to_logprob = {v:score for v,score in zip(vocab,log_probs)}\n",
    "\n",
    "top_vocab = sorted(vocab, key=v_to_logprob.get, reverse=True)[:5]\n",
    "print(top_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 - Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.73\n",
      "Recall: 0.20\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\b(?:' + '|'.join(re.escape(keyword) for keyword in top_vocab) + r')\\b'\n",
    "\n",
    "# apply regex to the test set\n",
    "matched = [i for i, text in enumerate(X_test) if re.search(pattern, text)]\n",
    "\n",
    "tp = sum(y_test[i] == 1 for i in matched)\n",
    "fp = len(matched) - tp\n",
    "fn = sum(y_test == 1) - tp\n",
    "\n",
    "# calculate precision and recall\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
